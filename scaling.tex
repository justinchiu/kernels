\documentclass{article}

\usepackage[a4paper,margin=1in]{geometry}

\usepackage{mystyle}
\usepackage[sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{mathrsfs}

\usepackage{appendix}

\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{subcaption}

\newcommand\Kexp{K_\textrm{exp}}
\newcommand\Kphi{K_\phi}
\newcommand\Krelu{K_\ReLU}

\title{Scaling Hidden Markov Language Models}

\begin{document}
\maketitle

\abstract{
Modern methods for language models based purely on neural networks are
opaque and difficult to analyze,
while alternative methods based on probabilistic graphical models
are interpretable but not performant.
We hypothesize that probabilistic methods are not as performant as neural methods
because they have not been scaled to massive sizes.
A significant barrier to scaling probabilistic methods is inference,
which typically takes computation superlinear in the size of the model.
In order to make probabilistic models performant while maintaining interpretability,
we present techniques for circumventing the heavy computational cost of inference.
We first focus on scaling one of the simplest models, hidden Markov models,
then show our techniques generalize to more complex models, such as 
probabilistic context-free grammars.
}

\section{Introduction}
Neural network-based generative models have led to progress in
difficult tasks such as language modeling and machine translation.
However, this progress comes at the cost of interpretability.
While neural networks are flexible function approximators,
they are also opaque and difficult to analyze.
This leads to difficulty in both trusting their predictions as well as
using them to validate scientific hypothesis.
There are three paths forward towards interpretability:
The first is post-hoc test-time analysis of models \citep{tenney2019probing},
the second is training-time augmentations of models \citep{},
and the third is model-based approaches \citep{}.
We explore the third path,
formulating models that interpretable from the start.
In particular, we focus on probabilistic graphical models,
which allow for the execution of arbitrary probabilistic queries.
These queries include
the ability to calculate conditional and marginal probabilities given evidence.
This interpretability comes at a cost:
Performing these queries via inference is expensive in both time and space.
The computational complexity of inference is a major impediment to scaling
graphical models to large sizes, which we hypothesize prevents them from
reaching the performance of neural networks.
We seek to answer the question of whether graphical models can be both
performant and interpretable.

We use language modeling as a benchmark task, as the complex 
and sometimes long-range phenomena in natural language provides a good testbed.
%Additionally, language modeling captures import scientific and linguistic questions.
Language model benchmarks are currently dominated by autoregressive neural models,
which makes it difficult to analyze what properties of language the models are
actually capturing.

In this work, we investigate scaling probabilistic graphical models
which explicitly reason about latent variables.
We focus on the one of the simplest latent variable models,
the hidden Markov model (HMM).
Despite the simplicity of HMMs,
they are still computationally expensive to scale
due to the cost of inference.
In order to scale HMMs, we utilize a kernel-based generalized softmax that greatly
reduces the complexity of inference.
Before diving into the generalized softmax, we first review HMMs.

%In order to scale HMMs, we propose a series of choices in parameterization
%that result in computational speedups for inference in HMMs:
%sparse emission constraint, state dropout, and an efficient kernel-based generalized softmax.
%Before diving into these techniques, we first review HMMs.


\section{Hidden Markov Models for Language Modeling}
Hidden Markov models (HMMs) have a rich history in natural language processing.
They have been used for a series of tasks, including
speech recognition \citep{rabiner1990tut},
part of speech tagging \citep{merialdo1994tagging}, 
and word alignment in machine translation \citep{vogel1996hmm}.
They have also been used as components in neural models,
such as in attention \citep{shankar2018posterior}.
We focus on applying HMMs to the task of language modeling,
where the goal is to model the sequence of tokens in a sentence $x = (x_1,\ldots,x_T)$.

HMMs are one of the simplest latent variable models for language modeling
with a per-token latent variable.
The simplicity is a result of the strong conditional independence assumptions:
Every timestep has a single discrete state variable used to represent context,
which is then the only information used to emit a token.
Additionally, the next state is a function of only the previous state.
These independence assumptions result in a bottleneck where all information
must flow through the single state variable at each timestep,
causing performance to be limited by the number of states.
Limiting dependencies in this way also gives the model interpretability.
(Expand on what that means)

Formally, HMMs have the following generative process:
for each $t \in [T]$ timestep, first choose a latent state $z_t \in \mcZ$,
where $|\mcZ|$ is the number of latent states,
then choose a token to emit $x_t \in \mcX$.
This defines the joint distribution:
\begin{equation}
\label{eqn:joint}
p(x,z) = \prod_t p(x_t\mid z_t) p(z_t \mid z_{t-1}),
\end{equation}
with transitions $p(z_t \mid z_{t-1})$, emissions $p(x_t \mid z_t)$,
%with transitions $[A]_{z_{t-1},z_t} = p(z_t \mid z_{t-1})$,
%emissions $[O]_{z_t,x_t} = p(x_t \mid z_t)$,
and a distinguished start state $z_0 = S$.

Training an HMM requires marginalizing over the unobserved
sequence $z = (z_1,\ldots,z_T)$ in order to obtain the evidence, $p(x) = \sum_z p(x,z)$,
via the forward algorithm.
The forward algorithm can be written as a sequence of matrix-vector multiplications:
Let the start vector, $\bm\pi\in\R^{|\mcZ|}$, have entries
\begin{equation}
\label{eqn:start}
[\bm\pi]_{z_1} = p(x_1 \mid z_1) p(z_1 \mid z_0=S),
\end{equation}
and the subsequent transition operators, $\Lambda_t\in\R^{|\mcZ|\times|\mcZ|}$ for $t\in[2,\ldots,T]$, 
have entries
\begin{equation}
\label{eqn:trans}
[\Lambda_t]_{z_{t-1},z_{t}} = p(x_t\mid z_t)p(z_t\mid z_{t-1}).
\end{equation}
The evidence is then given by
\begin{equation}
\label{eqn:evidence}
p(x) = \bm\pi^\top\Lambda_2\cdots\Lambda_T\mathbf{1},
\end{equation}
where $\mathbf{1}\in\R^{|\mcZ|\times1}$ is the column vector of all ones.

On a serial machine, the cost of computing each matrix-vector product in
the above equation takes time $O(|\mcZ|^2)$, resulting in a total
running time of $O(T|\mcZ|^2)$ for the forward algorithm.
The quadratic dependence on the number of states precludes
scaling to extremely large state spaces.

In the following sections we will introduce techniques that
aim to reduce the dependence of the time and space complexity
of inference on the size of the state space.

\section{Generalized Softmax with Kernels}
Parameterization of the probability distributions in a model affects both
computational performance as well as generalization.
Recent work on efficient attention in neural models shows that careful choice of parameterization
of the softmax kernel, the main component of attention,
results in large computational gains at little cost in accuracy
\citep{choromanski2020performer,peng2021rfa}.
In this section we will cover generalized softmax with kernels and 
its use in efficient inference.

Softmax is commonly used to parameterize conditional distributions in
neural nets, and has origins in smooth
approximations of argmax as well as the conditional max entropy problem \citep{}.
Focusing on the transition distribution $p(z_t \mid z_{t-1})$,
the softmax parameterization is as follows: 
\begin{equation*}
p(z_t \mid z_{t-1}) = \frac{\exp(\bu_{z_t-1}^\top \bv_{z_{t-1}})}
{\sum_{z'} \exp(\bu_{z_{t-1}}^\top \bv_{z'})},
\end{equation*}
with $\bu_{z_{t-1}},\bv_{z_t}\in\R^d$ vector-space embeddings of values of
$z_{t-1},z_{t}$.\footnote{
Softmax also has a temperature parameter that rescales 
the dot product, which we omit.
}
The numerator, $\exp(\bu_{z_{t-1}}^\top \bv_{z_t})$,
is the exponential kernel \citep{rawat2019sampledsoftmax}.

Although the parameterization of softmax is specific to the exponential kernel,
softmax can be generalized with arbitrary nonnegative kernels $K: \R^d\times\R^d\to\R_+$.
Replacing the exponential kernel with the nonnegative kernel $K$ yields 
generalized softmax,
\begin{equation}
\label{eqn:gensoftmax}
p(z_t \mid z_{t-1}) = \frac{K(\bu_{z_{t-1}}, \bv_{z_t})}{\sum_{z'} K(\bu_{z_{t-1}}, \bv_{z'})},
\end{equation}
where we again have embeddings $\bu_{z_{t-1}},\bv_{z_t}\in\R^d$.\footnote{
The term generalized softmax is inspired by generalized linear models,
which generalized linear models to non-Gaussian errors with a link function,
similar to the use of kernels in generalized softmax.
}

Kernels have a long history in machine learning,
where they are used to extend linear classification models to nonlinear feature spaces,
such as in support vector machines or regression.
In those settings, kernels are intuitively used to measure the similarity between two data points,
hinging on a connection to an inner product in a reproducing kernel Hilbert space.
%relying on the existence of a feature mapping in an associated reproducing kernel Hilbert space 
%for kernel functions with the finitely positive semi-definite property \citep{}.\todo{
%Can expand on this or remove.}
In the generalized softmax setting, kernels instead parameterize conditional distributions by
measuring the propensity of one random variable taking a particular value given
the value of another random variable.
However, the connection to inner products in reproducing kernel Hilbert spaces 
is still useful.
In particular, the connection guarantees the existence of a feature map $\phi:\R^d\to\mathbb{F}$,
a map from the input space $\R^d$ to the feature space $\mathbb{F}$,
which allows us to write generalized softmax in matrix form:
\begin{equation}
\label{eqn:gentrans}
p(z_t \mid z_{t-1}) = [\textrm{diag}(\mathbf{d})\phi(U)\phi(V)^\top]_{z_{t-1},z_t},
\end{equation}
where we have the stacked embeddings $\phi(U),\phi(V)\in\R^{|\mcZ|\times\dim(\mathbb{F})}$
and the normalizing constants $\mathbf{d}\in\R^{|\mcZ|}$,
\begin{align*}
\phi(U) &= [\phi(\bu_1)^\top, \phi(\bu_2)^\top, \ldots] \in\R^{|\mcZ|\times \dim(\mathbb{F})},\\
\phi(V) &= [\phi(\bv_1)^\top, \phi(\bv_2)^\top, \ldots]\in\R^{|\mcZ|\times \dim(\mathbb{F})},\\
[\mathbf{d}]_{z_{t-1}} &= \frac{1}{\phi(\bu_{z_{t-1}})^\top \sum_z\phi(\bv_z)}.
\end{align*}
We will rely on the dimension of the feature space being small 
relative to the number of states, $\dim(\mathbb{F}) < |\mcZ|$,
in order to improve the time and space complexity of inference.\footnote{
Matrix multiplication with a diagonal matrix is equivalent to rescaling rows,
and is therefore computationally cheap.
}

\section{Inference with Generalized Softmax}
Unfortunately, in the case of softmax, which relies on the exponential kernel,
the feature space $\mathbb{F}_{\exp}$ has infinite dimension.
This can be seen from the Taylor expansion:
$$
\exp(\bu^\top \bv)
= \sum_{n=0}^\infty \frac{(\bu^\top \bv)^n}{n!}
= \sum_{n=0}^\infty \frac{(\sum_{i=1}^d[\bu]_i[\bv]_i)^n}{n!}
= \sum_{n=0}^\infty \frac{\sum_{\mathbf{j} \in [d]^n}
    (\prod_{i=1}^n [\bu]_{[\mathbf{j}]_i})
    (\prod_{i=1}^n [\bv]_{[\mathbf{j}]_i})}{n!},
$$
where we have expanded the terms of the numerator by summing over all subsets
$\mathbf{j}$ of size $n$ elements of $[d]$, including repeats and different
orderings.
This yields a feature map of 
$$[\phi(\bu)]_{n,\mathbf{j}} = \frac{\prod_{i=1}^n [\bu]_{[\mathbf{j}]_i}}{\sqrt{n!}}$$
for all $n\in\N$.\footnote{
While this is one possible feature map, there may exist
multiple valid feature maps.
}
We can therefore express the exponential kernel as an inner product of
infinite dimensional vectors \citep{cotter2011gausskernel},
$$\exp(\bu^\top\bv) 
= \sum_{n=0}^\infty\sum_{\mathbf{j}\in[d]^n}[\phi(\bu)]_{n,\mathbf{j}}[\phi(\bv)]_{n,\mathbf{j}}
= \langle \phi(\bu), \phi(\bv) \rangle,
$$
where the dimension of the feature space $\dim(\mathbb{F}_{\exp}) = \dim(\phi(\bu)) = \infty$.
Since the infinite dimensional feature space of the exponential kernel is larger than
the finite $|\mcZ|$, recent work has instead turned to approximations of the exponential kernel
based on random Fourier features
\citep{rawat2019sampledsoftmax,choromanski2020performer,peng2021rfa}.
These approaches use a finite-dimensional unbiased estimator of the exponential kernel.

We propose to take a different strategy, where we directly learn a feature map
for the generalized softmax while disregarding the explicit functional form of the kernel.
This allows us to use the matrix representation of generalized softmax to 
efficiently perform inference.

Previously, Equation~\ref{eqn:evidence} shows how to compute the evidence,
$p(x)$, with a series of matrix-vector products.
This was done by first defining the transition operator,
$\Lambda_t$ as in Equations~\ref{eqn:trans} and \ref{eqn:start}.
We can perform a similar procedure with the transition distribution using generalized softmax.
With a generalized softmax parameterization of the transition distribution
and a feature space $\mathbb{F}_\phi$ with feature map $\phi$ and finite dimension $\dim(\mathbb{F})$,
we have the transition operators for all $t \in [2, \ldots, T]$,
\begin{equation}
\label{eqn:genlambda}
\Lambda_t =
\underbrace{\textrm{diag}(\mathbf{d})\phi(U)}_{L_t}\underbrace{{\phi(V)}^\top\textrm{diag}(\mathbf{o}_t)}_{R_t},
\end{equation}
where the entries of $\mathbf{o}_t\in\R^{|\mcZ|}$
are given by
$$
[\mathbf{o}_t]_{z_t} = p(x_t \mid z_t),
$$
the emission probability and $\mathbf{d}\in\R^{|\mcZ|}$ contains the normalizing constant
for each state.
We can interpret $L_t\in\R^{|\mcZ|\times\dim{\mathbb{F}}}$ as a 
projection into feature space, and $R_t\in\R^{\dim(\mathbb{F})\times|\mcZ|}$
as a projection back into state space.
The evidence can then be computed using $L_t$ and $R_t$ as follows:
\begin{equation}
\label{eqn:genevidence}
p(x)
= \bm\pi\Lambda_2\cdots\Lambda_T\mathbf{1}
= \bm\pi(L_2R_2)\cdots (L_TR_T)\mathbf{1}
= \bm\pi L_2R_2\cdots L_TR_T\mathbf{1},
\end{equation}
where we have substituted Equation~\ref{eqn:genlambda} into the computation of the evidence
and applied the associative property of matrix multiplication.
When performed from left to right, the matrix vector products now take $O(|\mcZ|\dim(\mathbb{F}))$
time, rather than $O(|\mcZ|^2)$,
with the overall complexity of inference now linear in the number of states,
$O(T|\mcZ|\dim(\mathbb{F}))$,
with generalized softmax.

We parameterize the feature map $\phi(\bu) = \exp(W_\phi\bu - \frac{\|\bu\|_2^2}{2})$,
with paramater $W_\phi\in\R^{\dim(\mathbb{F})\times d}$.
We then train all parameters,
including $W_\phi$ and the parameters of the transition and emission distributions,\footnote{
We give the full parameterization of all distributions in the appendix.
}
by optimizing the evidence with gradient ascent.

\section{Connection to Kernel Mean Embeddings and Kernel Belief Propagation}
A related line of work uses kernel methods to extend belief propagation.
Belief propagation is commonly constrained to models with parametric assumptions such as
small, discrete or Gaussian conditional distributions.
Kernel belief propagation (KBP) is a method that utilizes reproducing kernels
to run inference without the common parametric assumptions \citep{song2010tree,song2011kernelbp}.
At a high level, KBP rewrites the messages in belief propagation as conditional
expectations, then applies kernel-based conditional mean operators
to approximate the conditional expectations nonparametrically.
Whereas generalized softmax can be used to directly parameterize conditional distributions,
KBP instead relies on feature space operators in a manner that resembles Bayes' rule.
The generality of KBP comes at a cost: in order to avoid parametric assumptions
the operations in KBP are more expensive than generalized softmax.

In this section, we will derive the kernel conditional mean operator
used in KBP and compare it to generalized softmax.
However, before covering the conditional mean operator, we first
detail the kernel mean embedding.
Rather than computing conditional expectations,
the simpler kernel mean embedding computes unconditional expectations.
The kernel mean embedding extends the reproducing property,
which says that function application can be expressed as an inner product,
by showing that the expected values of functions of $x$ can also
be computed via inner products.

Given a nonnegative kernel function $k:\mcX\times\mcX\to\R_+$,\footnote{
A strictly positive kernel function must have the finitely positive semidefinite
kernel matrix property, allowing us to apply the Moore-Aronszajn theorem,
which constructively guarantees the existence of an associated RKHS.
}
there exists an associated reproducing kernel Hilbert space (RKHS) $\mathbb{F}$
which we call the feature space.\footnote{
Check if we need separability.
}
The RKHS has a feature mapping, $\phi_x:\mcX\to\mathbb{F}$,
such that, for all functions $f\in\mathbb{F}$,
function application can be expressed with the inner product
$\langle\cdot,\cdot\rangle_\mathbb{F}:\mathbb{F}\times\mathbb{F}\to\R_+$:
$$f(x) = \langle f, \phi_x(x)\rangle_\mathbb{F}.$$
This is known as the reproducing property.\footnote{
Show reproducing property does not hold for $L_2$?
This is known as the reproducing property of $\phi(x)\in\mathbb{F}$.
Terminology of reproducing property vs point evaluation property is murky.
Pick one and stay consistent.
}
We will omit the space associated with the inner product as
well as the subscript of the feature mapping when it is clear from context.

\subsection{Kernel Mean Embedding}
The kernel mean embedding \citep{smola2007}, $\mu_x\in\mathbb{F}$,
associated with the $\mcX$-valued random variable $x$
is defined as
\begin{equation}
\label{eqn:kme}
\mu_x = \Es{x}{\phi(x)}.
\end{equation}
It has the property that, for all $f \in \mathbb{F}$,
\begin{equation}
\label{eqn:expectation-inner-product}
\Es{x}{f(x)}
= \Es{x}{\langle f, \phi(x)\rangle}
= \langle f, \Es{x}{\phi(x)}\rangle
= \langle f, \mu_x \rangle
\end{equation}
by the reproducing property and linearity of expectation.
(to be super rigorous i think you have to show expectation is a self-adjoint linear operator)
This extends the reproducing property,
which says that function application can be expressed as an inner product,
by showing that the expected values of functions of $x$ can also
be computed via inner products. (repeated)

\textbf{Example}
Let $x\sim N(\mu, \Sigma)$ be a $d$-dimensional Gaussian,
feature mapping $\phi(x) = x,$
and inner product $\langle x, y\rangle = x^\top y$.
The kernel mean embedding of $x$ is given by $\mu_x = \Es{x}{\phi(x)} = \mu$.
Let function $f(x) = w^\top x,$ with $w\in\R^d$.
We then have $\Es{x}{f(x)} = \langle w, \mu_x \rangle = w^\top \mu$.

\textbf{Example}
Let $x\sim \textrm{Cat}(\theta)$ be a $d$-dimensional categorical distribution,
the one-hot feature mapping $\phi(x) = e_x\in\R^d,$
and inner product $\langle x, y\rangle = x^\top y$.
The kernel mean embedding of $x$ is then given by $\mu_x = \E{\phi(x)} = \theta$.
We can then model a single instance of attention as an expectation under this framework.
Let the function $f(x) = \bu_x\in\R^n$ map values of $x$ to their corresponding embedding.
With some abuse of notation,
given the stacked $f = [\bu_1, \bu_2, \ldots, \bu_d] \in \R^{d\times n}$,
where $n$ is the embedding dimension,
we then have
$\Es{x}{f(x)} = \langle f, \mu_x\rangle = f^\top\theta$.\footnote{
The expectation is applied to each dimension of the codomain of $f$ separately.
}
\todo{Make sure notation is consistent}
\todo{Also potentially clarify that no speedups are possible here due to only a single
instance of attn. Performer/RFA relies on conditional expectation}

Both of the above examples assume the mean parameters are known,
which requires parametric assumptions on the random variable $x$.
\footnote{This can be extended to the nonparametric setting by estimating the
kernel mean embedding via samples,
$\mu_x = \E{\phi(x)} \approx \frac{1}{n}\sum_{i=1}^n \phi(x_i), x_i\iid p(x)$.}

\subsection{Conditional Mean Embedding}
We can then extend the kernel mean embedding to the conditional mean embedding
by introducing covariance operators, which offer an alternative
representation for covariance using Hilbert spaces \citep{baker1970,baker1973}.
Given $\mcX$ and $\mcY$-valued RVs $x,y$, associated
RKHS $\mathbb{F}_x,\mathbb{F}_y$, and respective feature mappings
$\phi_x:\mcX\to\mathbb{F}_x,\phi_y:\mcY\to\mathbb{F}_y$.
We omit the feature mapping subscripts when they are clear from context.
Let the outer product $(g \otimes h)f = \langle f,h\rangle g$ for elements
$f,h\in\mathbb{F}_x$ and $g\in\mathbb{F}_y$.
The uncentered cross-covariance operator $C_{yx}: \mathbb{F}_x\to\mathbb{F}_y$ is defined by
\begin{equation}
    \label{eqn:xcov}
    C_{yx} = \Es{yx}{\phi_y(y)\otimes\phi_x(x)},
\end{equation}
for all $f\in\mathbb{H},g\in\mathbb{G}$ where $\mathbb{H},\mathbb{G}$ are
RKHSs over $\mcX,\mcY$.\footnote{
The uncentered terminology is a result of the definition of the (centered) cross-covariance
as $\E{(x - \E{x})(y - \E{y})} = \E{x\otimes y} - \E{x}\E{y}^\top$,
and the uncentered cross-covariance $\E{x\otimes y}$.
}\todo{different symbol for feature mapping for $y$}
The uncentered cross-covariance operator has the following property:
\begin{equation}
    \label{eqn:xcove}
    \begin{aligned}
    \langle g, C_{yx}f\rangle
    &= \langle g, \E{(\phi(y)\otimes\phi(x))}f\rangle && \text{(defn of }C_{yx})\\
    &= \E{\langle g, (\phi(y)\otimes\phi(x))f\rangle} && \text{(linearity of expectation)}\\
    &= \E{\langle g, \langle f, \phi(x)\rangle \phi(y)\rangle} && \text{(defn of }\otimes)\\
    &= \E{\langle f, \phi(x)\rangle\langle g, \phi(y)\rangle} && \text{(linearity of }\langle\cdot,\cdot\rangle)\\
    &= \Es{xy}{f(x)g(y)}, && \text{(reproducing property)}
    \end{aligned}
\end{equation}
which is an extension of the kernel mean embedding to a joint distribution.
The operator $C_{xx}:\mathbb{F}_x\to\mathbb{F}_x$ is called the covariance operator,
as it takes the form $C_{xx} = \Es{x}{\phi(x)\otimes\phi(x)}$.

\textbf{Example}
Let $(x,y)\sim N(0,\Sigma)$ bivariate Gaussian,
where $\Sigma_{xy} = \E{xy}$, the off-diagonal
entry of the covariance matrix.
If we take $\mathbb{F}_x=\mathbb{F}_y=\R^d$
with $\phi_x,\phi_y$ as the identity functions,
then $C_{xy} = \Sigma_{xy}$.

\textbf{Example}
Let $(x,y)\sim p(x,y)$, a discrete joint distribution,
with $\phi(x)=e_x,\phi(y)=e_y$ both mapping $x,y$ to one-hot representations.
This results in $[C_{xy}]_{x,y}=p(x,y)$.
If we also let $f=e_{x'},g=e_{y'}$ be one-hot representations of particular values of $x$ and $y$,
then $\langle f, C_{xy}g\rangle = \langle C_{yx}f, g\rangle= p(x=x',y=y')$.

Given the cross-covariance and covariance operators, $C_{yx}$ and $C_{xx}$,
we can now define the conditional mean operator.
The conditional mean operator uses the above covariance operators in a manner
similar to computing conditional probabilities via Bayes' rule: $p(y\mid x) = p(y,x)/p(x)$.
We define the conditional mean operator $U_{y\mid x}: \mathbb{F}_x\to\mathbb{F}_y$
as follows:
\begin{equation}
    \label{eqn:conditional-mean}
    U_{y\mid x} = C_{yx}C_{xx}^{-1}.
\end{equation}
The conditional mean operator allows conditional expectations to be
expressed as inner products.
For all $g \in \mathbb{F}_y$,
\begin{equation}
\label{eqn:conditional-expectation}
\langle g, U_{y\mid x}\phi(x) \rangle = \Es{y\mid x}{g(y)}.
\end{equation}
The correctness of this property can be shown as follows:
First, we apply the property of the covariance operator derived in
Equation~\ref{eqn:xcove}. For all $f\in\mathbb{F}_x$, we have
\begin{equation}
\label{eqn:prf-conditional-expectation}
    \langle f, C_{xx}\Es{y\mid x}{g(y)}\rangle
    = \Es{x}{f(x)\Es{y\mid x}{g(y)\mid x}}
    = \Es{xy}{f(x)g(y)}
    = \langle f, C_{xy}g\rangle.
\end{equation}
With some care,\footnote{
See the appendix of \citet{fukumizu2004kernel} for the full proof}
we can then multiply the second terms of the inner products on both sides
of the equation by the left inverse $C_{xx}^{-1}$.
This gives, forall $f\in\mathbb{F}_x$,
$$\langle f, \Es{y\mid x}{g(y)}\rangle = \langle f, C_{xx}^{-1}C_{xy}g \rangle
= \langle g, C_{yx}C_{xx}^{-1}f\rangle.$$
As this holds for any $f\in\mathbb{F}_x$, it also holds for $\phi_x$,
yielding
$$
\Es{y\mid x}{g(y)} = \langle g, U_{y\mid x}\phi(x)\rangle
$$
as desired, by the reproducing property.

\textbf{Example}
Consider discrete RVs $x$ and $y$, with $y\sim p(y\mid x)$.
We would like to compute $\Es{y\mid x}{g(y)}$ using the conditional mean operator.
Assuming $\mathbb{F}_x=\mathbb{F}_y=\R^d$,
with inner products
$\langle\cdot,\cdot\rangle_{\mathbb{F}_x}=\langle\cdot,\cdot\rangle_{\mathbb{F}_x}$
both equal to the dot product,
the conditional expectation is given by
$$\Es{y\mid x}{g(y)} = g^\top U_{y\mid x}\phi(x) = g^\top C_{yx}C_{xx}^{-1}\phi(x).$$
With this representation, it takes computation $O((|\mcY|+|\mcX|)d^2)$
to compute $C_{yx}$ and $C_{xx}$ and $O(d^3)$ to invert $C_{xx}$.
This is more expensive than generalized softmax,
which takes $O(|\mcY|d)$ computation.
The improved efficiency of generalized softmax comes from directly parameterizing
the conditional distribution $p(y\mid x)$.

(End current progress)
Is there a one sentence answer as to why we cannot estimate $p(y|x)$ with kernels directly?
Something like, in general it is hard to model conditional expectations, but
generalized softmax makes a particular assumption (tractable partition function,
closed form expression for mean) that makes it easy.

Similarly, inference in an HMM can also be written with conditional expectations.
At each timestep, the forward algorithm recursively computes the quantity
\begin{equation}
\label{eqn:alpha}
p(z_t \mid x_{<t})
= \sum_{z_{t-1}} p(z_t \mid z_{t-1})p(z_{t-1}\mid x_{<t})
= \Es{z_{t-1}\mid x_{<t}}{p(z_t \mid z_{t-1})}.
\end{equation}

Applying the definition of generalized softmax to Equation~\ref{eqn:alpha}
and rewriting $[\bm\alpha_t]_{z_t} = p(z_t \mid x_{\le t})$
we have
\begin{equation}
\bm\alpha_t = \textrm{diag}(d)\phi(U)\phi(V)^\top\bm\alpha_{t-1},
\end{equation}
by Equation~\ref{eqn:gentrans}.
Identical to the analysis in the previous section, this reduces the computation
performed at each timestep from $O(|\mcZ|^2)$ to $O(|\mcZ|\dim(\mathbb{F}))$.
This can then be used in conjunction with $p(x_t \mid z_t)$ to compute
the quantities
$$
p(x_t \mid x_{<t}) = \sum_{z_t}p(x_t \mid z_t)p(z_t \mid z_{<t}),
\qquad
p(z_t \mid x_{\le t}) = \frac{p(x_t, z_t \mid x_{<t})}{p(x_t \mid x_{<t})},
\qquad
p(x) = \prod_t p(x_t \mid x_{<t}),
$$
again yielding $O(T|\mcZ|\dim(\mathbb{F}))$ computation for computing the evidence.

However, the similarities between our work and KBP end there.
KBP focuses on using the kernel matrix representation for nonparametric inference
in non-Gaussian graphical models where exact computation of conditional expectations is
infeasible.
We eschew the explicit kernel representation and
instead use the inner product in feature space explicitly in order to get
computational speedups,
which is made possible by the tractability of the conditional expectation.

\section{Random Fourier Features}
Scaling factor of performer versus ours (length vs num features / embedding dimension).

Performer (protein): num features = 256, emb dim = 512, length = 12k?
MSE graph.
Performer (toy mse): num features = 128, emb dim = 16, length = 4096?
HMM: num features = length / 4, emb dim = 256

Performer and random feature attention find that num features does not scale with
length. How can we explain this?

Quote from performer:
`The dependence on the radius means that the length of queries and keys cannot grow
at a fixed number of features if we want to
retain the quality of the approximation.
In particular, this means that FAVOR cannot approximate
hard attention on sequences of unlimited length with a fixed number of features.'
Poorly written.
Interpretation: Random projections from embedding space to sphere.
If we increase the number of points in embedding space while keeping
their projections separated, we have to increase the volume of the sphere.

Question: why dont they see that in their experiments?
Possibly get away with a low degree of separation.

Explain argument from separation to entropy or rank.

\section{Structured Sparsity and Low Rank}
Since we are directly learning a low rank representation of the transition matrix,
it is impossible to find a higher rank solution.
If the best solution is of high rank, this is an issue.
We propose to increase the rank of the generalized softmax transition matrix
by instead parameterizing the transition matrix with a combination of both generalized softmax
as well as a banded matrix.

\subsection{Sum of Experts}
We constrain $\theta\in\R^{|\mcZ|\times|\mcZ|}$ to have banded structure,
such that $[\theta]_{z_{t-1},z_t} = 0$ if $|z_t - z_{t-1}| > K$.
Let $B = \set{(z_{t-1}, z_t): |z_t - z_{t-1}| \le K}$,
with $B_{z_{t}} = \set{z_{t-1}: |z_t - z_{t-1}| \le K}$
and $B_{z_{t-1}} = \set{z_t: |z_t - z_{t-1}| \le K}$
(these two are the same by symmetry).
\begin{equation}
\begin{aligned}
p(z_t \mid x_{<t})
&= \sum_{z_{t-1}}p(z_t \mid z_{t-1})p(z_{t-1} \mid x_{<t})\\
&= \sum_{z_{t-1}}
    \frac{[\theta]_{z_{t-1},z_{t}} + \phi(\bu_{z_{t-1}})^\top \phi(\bv_{z_{t}})}
    {Z_{z_{t-1}}} p(z_{t-1} \mid x_{<t})\\
&= \sum_{z_{t-1}\in B_{z_t}}
    \frac{[\theta]_{z_{t-1},z_{t}}}
    {Z_{z_{t-1}}} p(z_{t-1} \mid x_{<t})
+ \sum_{z_{t-1}} \frac{\phi(\bu_{z_{t-1}})^\top \phi(\bv_{z_{t}})}
    {Z_{z_{t-1}}} p(z_{t-1} \mid x_{<t}),
\end{aligned}
\end{equation}
with the normalizing constant
\begin{equation}
\begin{aligned}
Z_{z_{t-1}}
&= \sum_{z_{t}} [\theta]_{z_{t-1},z_{t}} + \phi(\bu_{z_{t-1}})^\top \phi(\bv_{z_{t}})\\
&= \underbrace{\sum_{z_{t}\in B_{z_{t-1}}} [\theta]_{z_{t-1},z_{t}}}_{Z_B} + 
    \underbrace{\phi(\bu_{z_{t-1}})^\top \sum_{z_t} \phi(\bv_{z_{t}})}_{Z_K}.
\end{aligned}
\end{equation}

\subsection{Product of Experts}
Using the same definition of $\theta$, we can instead use a product of experts parameterization.
\begin{equation}
\begin{aligned}
p(z_t \mid x_{<t})
&= \sum_{z_{t-1}}p(z_t \mid z_{t-1})p(z_{t-1} \mid x_{<t})\\
&= \sum_{z_{t-1}}
    \frac{[\theta]_{z_{t-1},z_{t}} \left(\phi(\bu_{z_{t-1}})^\top \phi(\bv_{z_{t}})\right)}
    {Z_{z_{t-1}}} p(z_{t-1} \mid x_{<t}),
\end{aligned}
\end{equation}
with the normalizing constant
\begin{equation}
\begin{aligned}
Z_{z_{t-1}}
&= \sum_{z_{t}} [\theta]_{z_{t-1},z_{t}} \phi(\bu_{z_{t-1}})^\top \phi(\bv_{z_{t}})
\end{aligned}
\end{equation}

\section{Emission Sparsity Constraints}

\section{Efficient Sampling}

\section{Spectral Methods}

\bibliographystyle{plainnat}
\bibliography{bib}

\begin{appendix}
\begin{comment}
\section{Gradient Estimator Implementation}
In the log-semiring, addition is given by $\bigoplus = \LSE$ and multiplication
by $\bigotimes = +$.
Consider the linear chain CRF, $\bigotimes_t \psi(x_{t-1}, x_t)$,
with $\psi(x_{t-1}, t) = f(x_t) \oplus g(x_{t-1},x_t)$.
We would like to compute the gradient of the
log partition function, $A = \bigoplus_x \bigotimes_t \psi(x_{t-1}, x_t)$.
Recall the gradient identities 
\begin{equation}
\begin{aligned}
\nabla_a a \bigoplus b &= \frac{\exp(a)}{\exp(a \bigoplus b)}\\
\nabla_a a \bigotimes b &= 1.
\end{aligned}
\end{equation}

We then have 
\begin{equation}
\begin{aligned}
\nabla_{\psi(x_a,x_b)} &\bigoplus_t \psi(x_{t-1}, x_t)\\
&= \nabla_{\psi(x_a,x_b)} \bigoplus_t \psi(x_{t-1}, x_t)\\
\end{aligned}
\end{equation}
\end{comment}

\end{appendix}

\end{document}
