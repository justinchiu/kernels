\documentclass{article}

\usepackage[a4paper,margin=1in]{geometry}

\usepackage{mystyle}
\usepackage[sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{algorithm}

\usepackage{appendix}

\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{subcaption}

\newcommand\Kexp{K_\textrm{exp}}
\newcommand\Kphi{K_\phi}
\newcommand\Krelu{K_\ReLU}

\title{Scaling Hidden Markov Language Models}

\begin{document}
\maketitle

\abstract{
Modern methods for language models based purely on neural networks are
opaque and difficult to analyze,
while alternative classical methods based on probabilistic graphical models
are interpretable but not performant.
We hypothesize that classical methods are not as performant because they have
neither been scaled to similar sizes as their modern neural counterparts nor taken advantage of 
modern parallel hardware.
A significant barrier to scaling in classical methods is inference,
which typically scales at best polynomially in the size of the model.
In order to make classical methods performant while maintaining interpretability,
we present techniques for circumventing the heavy computational costs of inference.
We first focus on scaling one of the simplest models, hidden Markov models,
then show our techniques generalize to more complex models, such as 
probabilistic context-free grammars.
}

\section{Introduction}
Neural network-based generative models have led to progress in
difficult tasks such as language modeling and machine translation.
However, this progress comes at the cost of interpretability.
Neural networks are flexible function approximations,
but that flexibility results in opaque models that are difficult to analyze.
Rather than post-hoc analysis \citep{tenney2019probing}, we instead propose to explore
the space of models that are defined with interpretability in mind.
In particular, we focus on probabilistic graphical models,
which allow for the execution of arbitrary probabilistic queries,
i.e. the calculation of (functions of) probability distributions
via operations such as conditioning and marginalization \citep{koller2009probabilistic} (simplify).
We seek to answer the question of whether we can have models that are both
performant and interpretable.

We use language modeling as a benchmark task, as the complex 
and sometimes long-range phenomena in natural language provides a good testbed.
Additionally, language modeling captures import scientific and linguistic questions
(more on this later).
Language model benchmarks are currently dominated by autoregressive neural models,
which makes it difficult to analyze what properties of language the models are
actually capturing.
For example, it is difficult to analyze the inner representations used
in syntax-inspired neural language models \citep{du2020exploiting}
(this is my own take on that paper,
need to find a better citation). 
This is an issue if the goal is to learn from models in order to answer
scientific questions.

In this work, we investigate scaling probabilistic graphical models
which explicitly reason about latent variables.
We focus on the one of the simplest latent variable models,
the hidden Markov model (HMM).
HMMs posit a simple generative process, that first generates a sequence of
latent states then the emissions.
Additionally, HMMs make very strong conditional independence assumptions.
Despite the simplicity and constraints of HMMs,
they are still computationally expensive to scale
due to the cost of marginalization.

In order to scale HMMs, we propose a series of choices in parameterization
that result in computational speedups for inference in HMMs:
sparse emission constraint, state dropout, and softmax kernel feature map approximation.

Before diving into these contributions, we first review HMMs.

\section{Hidden Markov Models for Language Modeling}
Hidden Markov models (HMMs) have a rich history in natural language processing.
They have been used for a series of tasks, including
speech recognition \citep{rabiner1990tut},
part of speech tagging \citep{merialdo1994tagging}, 
and word alignment in machine translation \citep{vogel1996hmm}.
They have also been used as components in neural models,
such as in attention \citep{shankar2018posterior}.
We focus on applying HMMs to the task of language modeling,
where the goal is to model the tokens in a sentence $x = (x_1,\ldots,x_T)$.

HMMs are one of the simplest latent variable models for language modeling
with a per-token latent variable.
The simplicity is a result of the very strong conditional independence assumption:
Every timestep has a single discrete state latent variable for representing context,
and HMMs emit each token given only the corresponding latent state.
Additionally, the next state is a function of only the previous state.
These independence assumptions result in a bottleneck where all information
must flow through the single state variable,
causing performance to be limited by the number of states
while also giving the model interpretability by limiting dependencies.

Formally, HMMs have the following generative process:
for each $t \in [T]$ timestep, first choose a latent state $z_t \in \mcZ$,
where $|\mcZ|$ is the number of latent states,
then choose a token to emit $x_t \in \mcX$.
This defines the joint distribution:
\begin{equation}
p(x,z) = \prod_t p(x_t\mid z_t) p(z_t \mid z_{t-1}),
\end{equation}
with transitions $p(z_t \mid z_{t-1})$, emissions $p(x_t \mid z_t)$,
and a distinguished start state $z_0 = S$.
%where we have the transition matrix $A_{z_{t-1}z_t} = p(z_t \mid z_{t-1})$
%and emission matrix $O_{z_tx_t} = p(x_t \mid z_t)$.

Training an HMM requires marginalizing over the unobserved
$z = (z_1,\ldots,z_T)$ in order to obtain the evidence $p(x) = \sum_z p(x,z)$,
accomplished via the forward algorithm.
The forward algorithm can be written as a sequence of matrix-vector multiplications:
Let the transition operators $\Lambda_t\in\R^{|\mcZ|\times|\mcZ|}$ for $t\in[2,\ldots,T]$, 
with entries $$[\Lambda_t]_{z_t,z_{t-1}} = p(x_t\mid z_t)p(z_t\mid z_{t-1}),$$
with the first operator given by
$$[\Lambda_1]_{z_1,z_0} = \begin{cases}
p(x_1 \mid z_1) p(z_1 \mid z_0=S) & z_0 = S\\
0 & \textrm{otherwise}.
\end{cases}
$$
The evidence is then given by
\begin{equation}
p(x) = \mathbf{1}^\top\Lambda_1\Lambda_2\cdots\Lambda_T\mathbf{1},
\end{equation}
where $\mathbf{1}\in\R^{|\mcZ|\times1}$ is the column vector of all ones.

On a serial machine, the cost of computing each matrix-vector product in
the above equation takes time $O(|\mcZ|^2)$, resulting in a total
running time of $O(T|\mcZ|^2)$ for the forward algorithm.
\todo{gradient computation requires $O(T|\mcZ|^2)$ space}
The quadratic dependence on the number of states precludes
scaling to extremely large state spaces.

\section{Kernel Parameterization}
Recent work in latent variable modeling with neural components has shown
that the parameterization of conditional distributions can have a large
effect on generalization \citep{kim2019cpcfg}. 
In particular, one is able to encode different inductive biases through choices in
parameterization.

A common choice when parameterizing conditional distributions with
neural nets is the use of softmax, which has origins in smooth
approximations of argmax as well as the conditional max entropy problem (citations needed).
In a conditional distribution $p(x \mid z)$, the softmax parameterization is as follows: 
$$p(x \mid z) = \frac{\exp(\bu_z^\top \bv_x)}{\sum_{x'} \bu_z^\top \bv_{x'})},$$
with $\bu_z,\bv_x\in\R^d$ vector-space embeddings of values of $x,z$.
The numerator, $\exp(\bu_z^\top \bv_x)$, is an instance of a kernel,
referred to as the exponential kernel \citep{rawat2019sampledsoftmax}.

Kernel methods have a rich history in machine learning,
having been applied to nonparametric regression, support vector machines, gaussian processes,
among other models, to a large degree of success.
In those settings, kernels are intuitively used to measure the similarity between two data points.
In our setting, kernels parameterize conditional distributions,
measuring the propensity of one random variable taking a particular value given
the value of another.

Generalizing from the exponential kernel,
a kernel is a function $K: \R^d\times\R^d\to\R$.
Although applications that rely on the kernel trick, such as support vector machines,
only require positive symmetric definite kernels,
our use of kernels to define probability mass functions additionally requires
kernels to be positive, with $K: \R^d\times\R^d\to\R_+$.
We can then replace the exponential kernel in softmax with
an arbitrary positive kernel:
$$p(x \mid z) = \frac{K(\bu_z, \bv_x)}{\sum_{x'} K(\bu_z, \bv_{x'})},$$
where we again have embeddings $\bu_z,\bv_x\in\R^d$.

(Can include more details if necessary later, or remove)
A positive kernel must have a positive definite kernel matrix,
which, by the Moore-Aronszajn theorem \citep{aronszajn}, implies that there exists a 
reproducing kernel Hilbert space (RKHS) and associated feature map.
(insert theorem here)

In the case of the exponential kernel $\Kexp$, the feature map has infinite dimension.
This can be seen from the Taylor expansion:
$$\exp(\bu^\top \bv) = \sum_{n=0}^\infty \frac{(\bu^\top \bv)^n}{n!},$$
yielding a feature map of 
$$[\phi(\bu)]_n = \frac{\prod_i}{\sqrt{n!}},$$
for all $n\in\N$.
one possible feature map, exists multiple valid maps
\citep{cotter2011gausskernel}.


Consider the emission distribution $p(x \mid z; \theta)$,
with discrete random variables $x,z$ and model parameters $\theta$.\footnote{
The dimension of the model parameters $\theta$ will vary depending on the choice
of parameterization.}\footnote{
The following parameterizations extend to the transition distribution as well,
and any conditional distribution.}
The full rank parameterization defines the model parameters $\theta\in\R^{|\mcZ|\times|\mcX|}$,
giving a one-to-one correspondence between the distribution parameters and model parameters:
$$p(x=j \mid z=i) = [\theta]_{i,j}.$$
%Writing the emission matrix as $O$, with entries $[O]_{ij} = p(x=j\mid z=i)$,
%we have $O = \theta$.

A kernel parameterization instead defines model parameters $\theta = \set{U,V}$,
with $U\in\R^{|\mcZ|\times d},V\in\R^{|\mcX|\times d}$,\footnote{
Typically $U$ and $V$ are referred to as embedding matrices,
as the embeddings of the elements of $\mcZ$ and $\mcX$ in $\R^d$ are given by 
the rows of $U$ and $V$.
} and obtains the distributional
parameters via
$$p(x=j \mid z=i) = \frac{K(u_i, v_j)}{\sum_{j'} K(u_i, v_{j'})},$$
where $u_i$ and $v_j$ are the $i$th and $j$th rows of $U$ and $V$,
and $K: \R^d \times \R^d \to \R$ is a kernel.
There are a variety of valid kernels $K$ one could employ;
however, we focus on the exponential kernel, given by $K(u, v) = \exp(u^\top v)$,
as it is widely used in practice as part of the softmax function.

(turn into footnote, or experimental detail)
Finally, a neural parameterization takes a further step in abstraction by
parameterizing the embedding matrices $U,V$ as functions of neural networks,
allowing for more parameter sharing.
For the embedding matrix $U\in\R^{|\mcZ|\times d}$,
this entails having another underlying embedding matrix $U'\in\R^{|\mcZ| \times d'}$
generating the rows $u_i = f_U(u_i')$, where the same $f_U$ is used for all $i \in \mcZ$.
In this work, we use the following parameterization for $f$:
(resnet thing).

Pros and cons of each, implications?
Probably make this a subsection too.

Add summarizing paragraph about the different levels of abstraction, with a picture.
Talk about going up and down the stack?
This way when we talk about each technique we can point
to exactly what abstraction it operates in.

Why is softmax / exponential kernel special?
Maximum entropy distributions, exponential family.
\href{https://www.lix.polytechnique.fr/~nielsen/CIG-slides.pdf}{Slides on information projections}
Power series representation

Does this apply to other kernels? Is exp special? Just has to be positive.
Examine range of kernel?

\section{Kernel Approximations}
Taylor approximation, generating functions

limitations?
\subsection{Bochner Theorem}
\subsection{Nystrom}

\section{Kernelized Inference}

\section{Generalization: Kernelized Belief Propagation}

\section{Spectral Methods?}

\bibliographystyle{plainnat}
\bibliography{bib}

\begin{appendix}
\begin{comment}
\section{Gradient Estimator Implementation}
In the log-semiring, addition is given by $\bigoplus = \LSE$ and multiplication
by $\bigotimes = +$.
Consider the linear chain CRF, $\bigotimes_t \psi(x_{t-1}, x_t)$,
with $\psi(x_{t-1}, t) = f(x_t) \oplus g(x_{t-1},x_t)$.
We would like to compute the gradient of the
log partition function, $A = \bigoplus_x \bigotimes_t \psi(x_{t-1}, x_t)$.
Recall the gradient identities 
\begin{equation}
\begin{aligned}
\nabla_a a \bigoplus b &= \frac{\exp(a)}{\exp(a \bigoplus b)}\\
\nabla_a a \bigotimes b &= 1.
\end{aligned}
\end{equation}

We then have 
\begin{equation}
\begin{aligned}
\nabla_{\psi(x_a,x_b)} &\bigoplus_t \psi(x_{t-1}, x_t)\\
&= \nabla_{\psi(x_a,x_b)} \bigoplus_t \psi(x_{t-1}, x_t)\\
\end{aligned}
\end{equation}
\end{comment}

\end{appendix}

\end{document}
