\documentclass{article}

\usepackage[a4paper,margin=1in]{geometry}

\usepackage{mystyle}
\usepackage[sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{mathrsfs}

\usepackage{appendix}

\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{subcaption}

\newcommand\Kexp{K_\textrm{exp}}
\newcommand\Kphi{K_\phi}
\newcommand\Krelu{K_\ReLU}

\title{Scaling Hidden Markov Language Models}

\begin{document}
\maketitle

\abstract{
Modern methods for language models based purely on neural networks are
opaque and difficult to analyze,
while alternative methods based on probabilistic graphical models
are interpretable but not performant.
We hypothesize that classical methods are not as performant because they have
neither been scaled to similar sizes as their modern neural counterparts nor taken advantage of 
modern parallel hardware.
A significant barrier to scaling in classical methods is inference,
which typically scales superlinearly in the size of the model.
In order to make probabilistic models performant while maintaining interpretability,
we present techniques for circumventing the heavy computational costs of inference.
We first focus on scaling one of the simplest models, hidden Markov models,
then show our techniques generalize to more complex models, such as 
probabilistic context-free grammars.
}

\section{Introduction}
Neural network-based generative models have led to progress in
difficult tasks such as language modeling and machine translation.
However, this progress comes at the cost of interpretability.
Neural networks are flexible function approximations,
but that flexibility results in opaque models that are difficult to analyze.
As opposed to post-hoc analysis of models \citep{tenney2019probing},
we instead propose to explore
the space of models that interpretable from the start.
In particular, we focus on probabilistic graphical models,
which allow for the execution of arbitrary probabilistic queries.
These queries include
the ability to calculate conditional and marginal probabilities given evidence.
This interpretability comes at a cost.
Performing these queries via inference is expensive in both time and space.
The computational complexity of inference is a major impediment to scaling
graphical models to large sizes, which we hypothesize prevents them from
reaching the performance of neural networks.
We seek to answer the question of whether we can have graphical models that are both
performant and interpretable.

We use language modeling as a benchmark task, as the complex 
and sometimes long-range phenomena in natural language provides a good testbed.
%Additionally, language modeling captures import scientific and linguistic questions.
Language model benchmarks are currently dominated by autoregressive neural models,
which makes it difficult to analyze what properties of language the models are
actually capturing.
%For example, it is difficult to analyze the inner representations used
%in syntax-inspired neural language models \citep{du2020exploiting}
%(this is my own take on that paper,
%need to find a better citation). 
%This is an issue if the goal is to learn from models in order to answer
%scientific questions.

In this work, we investigate scaling probabilistic graphical models
which explicitly reason about latent variables.
We focus on the one of the simplest latent variable models,
the hidden Markov model (HMM).
HMMs posit a simple generative process, that first generates a sequence of
latent states then the emissions.
Additionally, HMMs make very strong conditional independence assumptions.
Despite the simplicity of HMMs,
they are still computationally expensive to scale
due to the cost of inference.

%In order to scale HMMs, we propose a series of choices in parameterization
%that result in computational speedups for inference in HMMs:
%sparse emission constraint, state dropout, and an efficient kernel-based generalized softmax.
%Before diving into these techniques, we first review HMMs.

In order to scale HMMs, we utilize a kernel-based generalized softmax that greatly
reduces the complexity of inference.
Before diving into the generalized softmax, we first review HMMs.

\section{Hidden Markov Models for Language Modeling}
Hidden Markov models (HMMs) have a rich history in natural language processing.
They have been used for a series of tasks, including
speech recognition \citep{rabiner1990tut},
part of speech tagging \citep{merialdo1994tagging}, 
and word alignment in machine translation \citep{vogel1996hmm}.
They have also been used as components in neural models,
such as in attention \citep{shankar2018posterior}.
We focus on applying HMMs to the task of language modeling,
where the goal is to model the tokens in a sentence $x = (x_1,\ldots,x_T)$.

HMMs are one of the simplest latent variable models for language modeling
with a per-token latent variable.
The simplicity is a result of the strong conditional independence assumptions:
Every timestep has a single discrete state variable used to represent context,
which is then the only information used to emit a token.
Additionally, the next state is a function of only the previous state.
These independence assumptions result in a bottleneck where all information
must flow through the single state variable at each timestep,
causing performance to be limited by the number of states.
Limiting dependencies in this way also gives the model interpretability.

Formally, HMMs have the following generative process:
for each $t \in [T]$ timestep, first choose a latent state $z_t \in \mcZ$,
where $|\mcZ|$ is the number of latent states,
then choose a token to emit $x_t \in \mcX$.
This defines the joint distribution:
\begin{equation}
\label{eqn:joint}
p(x,z) = \prod_t p(x_t\mid z_t) p(z_t \mid z_{t-1}),
\end{equation}
with transitions $p(z_t \mid z_{t-1})$, emissions $p(x_t \mid z_t)$,
%with transitions $[A]_{z_{t-1},z_t} = p(z_t \mid z_{t-1})$,
%emissions $[O]_{z_t,x_t} = p(x_t \mid z_t)$,
and a distinguished start state $z_0 = S$.

Training an HMM requires marginalizing over the unobserved
$z = (z_1,\ldots,z_T)$ in order to obtain the evidence, $p(x) = \sum_z p(x,z)$,
via the forward algorithm.
The forward algorithm can be written as a sequence of matrix-vector multiplications:
Let the transition operators $\Lambda_t\in\R^{|\mcZ|\times|\mcZ|}$ for $t\in[2,\ldots,T]$, 
with entries
\begin{equation}
\label{eqn:trans}
[\Lambda_t]_{z_{t-1},z_{t}} = p(x_t\mid z_t)p(z_t\mid z_{t-1}),
\end{equation}
with the start vector, $\pi\in\R^{|\mcZ|}$, given by
\begin{equation}
\label{eqn:start}
[\pi]_{z_1} = p(x_1 \mid z_1) p(z_1 \mid z_0=S).
\end{equation}
The evidence is then given by
\begin{equation}
\label{eqn:evidence}
p(x) = \bm\pi^\top\Lambda_2\cdots\Lambda_T\mathbf{1},
\end{equation}
where $\mathbf{1}\in\R^{|\mcZ|\times1}$ is the column vector of all ones.

On a serial machine, the cost of computing each matrix-vector product in
the above equation takes time $O(|\mcZ|^2)$, resulting in a total
running time of $O(T|\mcZ|^2)$ for the forward algorithm.
\todo{gradient computation requires $O(T|\mcZ|^2)$ space}
The quadratic dependence on the number of states precludes
scaling to extremely large state spaces.

In the following sections we will introduce techniques that
aim to reduce the dependence of the time and space complexity
of inference on the size of the state space.

\section{Generalized Softmax}
Parameterization of the probability distributions in a model affects both
computational performance as well as generalization.
Recent work on efficient attention in neural models shows that careful choice of parameterization
of the softmax kernel, the main component of attention,
results in large computational gains at little cost in accuracy
\citep{choromanski2020performer,peng2021rfa}.
In this section we will cover generalized softmax with kernels and the
efficient computation of conditional expectations with generalized softmax.

Softmax is commonly used to parameterize conditional distributions in
neural nets, and has origins in smooth
approximations of argmax as well as the conditional max entropy problem \citep{}.
Focusing on the transition distribution $p(z_t \mid z_{t-1})$,
the softmax parameterization is as follows: 
\begin{equation*}
p(z_t \mid z_{t-1}) = \frac{\exp(\bu_{z_t-1}^\top \bv_{z_{t-1}})}
{\sum_{z'} \exp(\bu_{z_{t-1}}^\top \bv_{z'})},
\end{equation*}
with $\bu_{z_{t-1}},\bv_{z_t}\in\R^d$ vector-space embeddings of values of
$z_{t-1},z_{t}$.\footnote{
Softmax also has a temperature parameter that rescales 
the dot product, which we omit.
}
The numerator, $\exp(\bu_{z_{t-1}}^\top \bv_{z_t})$,
is the exponential kernel \citep{rawat2019sampledsoftmax}.

Although the parameterization of softmax is specific to the exponential kernel,
softmax can be generalized with arbitrary nonnegative kernels $K: \R^d\times\R^d\to\R_+$.
Replacing the exponential kernel with the nonnegative kernel $K$ yields 
generalized softmax,
\begin{equation}
\label{eqn:gensoftmax}
p(z_t \mid z_{t-1}) = \frac{K(\bu_{z_{t-1}}, \bv_{z_t})}{\sum_{z'} K(\bu_{z_{t-1}}, \bv_{z'})},
\end{equation}
where we again have embeddings $\bu_{z_{t-1}},\bv_{z_t}\in\R^d$.\footnote{
The term generalized softmax is inspired by generalized linear models,
which generalized linear models to non-gaussian errors with a link function,
similar to the use of kernels in generalized softmax.
}

Kernels have a long history in machine learning,
where they are used to extend linear classification models to nonlinear feature spaces,
such as in support vector machines or regression.
In those settings, kernels are intuitively used to measure the similarity between two data points,
hinging on a connection to an inner product in a reproducing kernel Hilbert space.
%relying on the existence of a feature mapping in an associated reproducing kernel Hilbert space 
%for kernel functions with the finitely positive semi-definite property \citep{}.\todo{
%Can expand on this or remove.}
In the generalized softmax setting, kernels instead parameterize conditional distributions by
measuring the propensity of one random variable taking a particular value given
the value of another random variable.
However, the connection to inner products in reproducing kernel Hilbert spaces 
is still useful.
In particular, the connection guarantees the existence of a feature map $\phi:\R^d\to\mathbb{F}$,
a map from the input space $\R^d$ to the feature space $\mathbb{F}$,
which allows us to write generalized softmax in matrix form:
$$p(z_t \mid z_{t-1}) = [D\phi(U)\phi(V)^\top]_{z_{t-1},z_t},$$
where we have the stacked embeddings $\phi(U),\phi(V)$ and the normalizing constants $D$,
\begin{align*}
\phi(U) &= [\phi(\bu_1)^\top, \phi(\bu_2)^\top, \ldots] \in\R^{|\mcZ|\times \dim(\mathbb{F})},\\
\phi(V) &= [\phi(\bv_1)^\top, \phi(\bv_2)^\top, \ldots]\in\R^{|\mcZ|\times \dim(\mathbb{F})},\\
D &= \textrm{diag}([\phi(\bu_1)^\top \sum_z\phi(\bv_z), \phi(\bu_2)^\top \sum_z\phi(\bv_z),\ldots]).
\end{align*}
We will rely on the dimension of the feature space being small 
relative to the number of states, $\dim(\mathbb{F}) < |\mcZ|$,
in order to improve the time and space complexity of inference.

\section{Inference with Generalized Softmax}
Unfortunately, in the case of softmax, which relies on the exponential kernel,
the feature space $\mathbb{F}_{\exp}$ has infinite dimension.
This can be seen from the Taylor expansion:
$$
\exp(\bu^\top \bv)
= \sum_{n=0}^\infty \frac{(\bu^\top \bv)^n}{n!}
= \sum_{n=0}^\infty \frac{(\sum_{i=1}^d[\bu]_i[\bv]_i)^n}{n!}
= \sum_{n=0}^\infty \frac{\sum_{\mathbf{j} \in [d]^n}
    (\prod_{i=1}^n [\bu]_{[\mathbf{j}]_i})
    (\prod_{i=1}^n [\bv]_{[\mathbf{j}]_i})}{n!},
$$
where we have expanded the terms of the numerator by summing over all subsets
$\mathbf{j}$ of size $n$ elements of $[d]$, including repeats and different
orderings.
This yields a feature map of 
$$[\phi(\bu)]_{n,\mathbf{j}} = \frac{\prod_{i=1}^n [\bu]_{[\mathbf{j}]_i}}{\sqrt{n!}}$$
for all $n\in\N$.\footnote{
While this is one possible feature map, there may exist
multiple valid feature maps.
}
We can therefore express the exponential kernel as an inner product of
infinite dimensional vectors \citep{cotter2011gausskernel},
$$\exp(\bu^\top\bv) 
= \sum_{n=0}^\infty\sum_{\mathbf{j}\in[d]^n}[\phi(\bu)]_{n,\mathbf{j}}[\phi(\bv)]_{n,\mathbf{j}}
= \langle \phi(\bu), \phi(\bv) \rangle,
$$
where the dimension of the feature space $\dim(\mathbb{F}_{\exp}) = \dim(\phi(\bu)) = \infty$.
Since the infinite dimensional feature space of the exponential kernel is larger than
the finite $|\mcZ|$, recent work has instead turned to approximations of the exponential kernel
based on random Fourier features
\citep{rawat2019sampledsoftmax,choromanski2020performer,peng2021rfa}.
These approaches use a finite-dimensional unbiased estimator of the exponential kernel.

We propose to take a different strategy, where we directly learn a feature map
for the generalized softmax while disregarding the explicit functional form of the kernel.
This allows us to use the matrix representation of generalized softmax to 
efficiently perform inference.

Previously, Equation~\ref{eqn:evidence} shows how to compute the evidence,
$p(x)$, with a series of matrix-vector products.
This was done by first defining the transition operator,
$\Lambda_t$ as in Equations~\ref{eqn:trans} and \ref{eqn:start}.
We can perform a similar procedure with the transition distribution using generalized softmax.
With a generalized softmax parameterization of the transition distribution
and a feature space $mathbb{F}_\phi$ with feature map $\phi$ and finite dimension $\dim(\mathbb{F})$,
we have the transition operators for all $t \in [2, \ldots, T]$,
\begin{equation}
\label{eqn:genlambda}
\Lambda_t =
\underbrace{\textrm{diag}(\mathbf{o}_t) \phi(U)}_{L_t}\underbrace{{\phi(V)}^\top}_{R_t},
\end{equation}
where $\mathbf{o}_t = [
    p(x_t \mid z_t=1)\phi(\bu_1)^\top \sum_z\phi(\bv_z),
    p(x_t \mid z_t=2)\phi(\bu_2)^\top \sum_z\phi(\bv_z),
    \ldots
]\in\R^{|\mcZ|}$, the emission probability and normalizing constant for each state.
We can interpret $L_t\in\R^{|\mcZ|\times\dim{\mathbb{F}}}$ as a 
projection into feature space, and $R_t\in\R^{\dim(\mathbb{F})\times|\mcZ|}$
as a projection back into state space.
The evidence can then be computed using $L_t$ and $R_t$ as follows:
\begin{equation}
\label{eqn:genevidence}
p(x)
= \bm\pi\Lambda_2\cdots\Lambda_T\mathbf{1}
= \bm\pi(L_2R_2)\cdots (L_TR_T)\mathbf{1}
= \bm\pi L_2R_2\cdots L_TR_T\mathbf{1},
\end{equation}
where we have substituted Equation~\ref{eqn:genlambda} into the computation of the evidence
and applied the associative property of matrix multiplication.
When performed from left to right, the matrix vector products now take $O(|\mcZ|\dim(\mathbb{F}))$
time, rather than $O(|\mcZ|^2)$,
with the overall complexity of inference now linear in the number of states,
$O(T|\mcZ|\dim(\mathbb{F}))$,
with generalized softmax.

\subsection{Connection to Kernel Mean Embeddings and Kernel Belief Propagation}
(Not done yet, tbd)
Many probabilistic queries can be written as conditional expectations.
In particular, we can write marginalization in an HMM as a conditional expectation:
$$p(x) = \sum_z p(x,z) = \sum_z p(z)p(x \mid z) = \Es{z}{p(x \mid z)}.$$
Decomposing the expectation over time and examining a single timestep, we have
$$p(x_t \mid x_{<t})
= \sum_{z_t}p(x_t\mid z_t)p(z_t \mid x_{<t}).
$$
The second term, $p(z_t \mid x_{<t})$,
can be further decomposed using the generative process of the HMM to
$$
 p(z_t \mid x_{<t}) = \sum_{z_{t-1}}p(z_t \mid z_{t-1})p(z_{t-1} \mid x_{<t}),
$$
altogether yielding the equation
$$
p(x_t \mid x_{<t})
= \sum_{z_t} p(x_t \mid z_t)\sum_{z_{t-1}}p(z_t \mid z_{t-1})p(z_{t-1} \mid x_{<t})
= \Es{z_{t-1},z_t\mid z_{t-1}}{p(x_t \mid z_t)}p(z_{t-1}\mid x_{<t}).
$$
We can use the matrix form of generalized softmax to speed up this computation.
Let $[\mathbf{f}_t]_{z_t} = p(x_t\mid z_t), [\bm\alpha_t]_{z_{t-1}} = p(z_{t-1}\mid x_{<t})$ in
$$\Es{z_t\mid x_{<t}}{p(x_t\mid z_t)}=(\bm\alpha_t\circ \mathbf{d})^\top\phi(U)\phi(V)^\top\mathbf{f}_t,$$
where $\mathbf{d} = \frac{1}{\phi(U)\sum_z\phi(\bv_z)}\in\R^{|\mcZ|}$.


\section{Kernel Approximations}
Move to appendix

Taylor approximation, generating functions

limitations?
\subsection{Bochner Theorem}
\subsection{Nystrom}

\section{Kernelized Inference}

\section{Generalization: Kernelized Belief Propagation}

\section{Spectral Methods?}

\bibliographystyle{plainnat}
\bibliography{bib}

\begin{appendix}
\begin{comment}
\section{Gradient Estimator Implementation}
In the log-semiring, addition is given by $\bigoplus = \LSE$ and multiplication
by $\bigotimes = +$.
Consider the linear chain CRF, $\bigotimes_t \psi(x_{t-1}, x_t)$,
with $\psi(x_{t-1}, t) = f(x_t) \oplus g(x_{t-1},x_t)$.
We would like to compute the gradient of the
log partition function, $A = \bigoplus_x \bigotimes_t \psi(x_{t-1}, x_t)$.
Recall the gradient identities 
\begin{equation}
\begin{aligned}
\nabla_a a \bigoplus b &= \frac{\exp(a)}{\exp(a \bigoplus b)}\\
\nabla_a a \bigotimes b &= 1.
\end{aligned}
\end{equation}

We then have 
\begin{equation}
\begin{aligned}
\nabla_{\psi(x_a,x_b)} &\bigoplus_t \psi(x_{t-1}, x_t)\\
&= \nabla_{\psi(x_a,x_b)} \bigoplus_t \psi(x_{t-1}, x_t)\\
\end{aligned}
\end{equation}
\end{comment}

\end{appendix}

\end{document}
