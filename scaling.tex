\documentclass{article}

\usepackage[a4paper,margin=1in]{geometry}

\usepackage{mystyle}
\usepackage[sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{mathrsfs}

\usepackage{appendix}

\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{subcaption}

\newcommand\Kexp{K_\textrm{exp}}
\newcommand\Kphi{K_\phi}
\newcommand\Krelu{K_\ReLU}

\title{Scaling Hidden Markov Language Models}

\begin{document}
\maketitle

\abstract{
Modern methods for language models based purely on neural networks are
opaque and difficult to analyze,
while alternative methods based on probabilistic graphical models
are interpretable but not performant.
We hypothesize that classical methods are not as performant because they have
neither been scaled to similar sizes as their modern neural counterparts nor taken advantage of 
modern parallel hardware.
A significant barrier to scaling in classical methods is inference,
which typically scales at best polynomially in the size of the model.
In order to make probabilistic models performant while maintaining interpretability,
we present techniques for circumventing the heavy computational costs of inference.
We first focus on scaling one of the simplest models, hidden Markov models,
then show our techniques generalize to more complex models, such as 
probabilistic context-free grammars.
}

\section{Introduction}
Neural network-based generative models have led to progress in
difficult tasks such as language modeling and machine translation.
However, this progress comes at the cost of interpretability.
Neural networks are flexible function approximations,
but that flexibility results in opaque models that are difficult to analyze.
As opposed to post-hoc analysis of models \citep{tenney2019probing},
we instead propose to explore
the space of models that interpretable from the start.
In particular, we focus on probabilistic graphical models,
which allow for the execution of arbitrary probabilistic queries.
These queries include
the ability to calculate conditional and marginal probabilities given evidence.
This interpretability comes at a cost.
Performing these queries via inference is expensive in both time and space.
The computational complexity of inference is a major impediment to scaling
graphical models to large sizes, which we hypothesize prevents them from
reaching the performance of neural networks.
We seek to answer the question of whether we can have graphical models that are both
performant and interpretable.

We use language modeling as a benchmark task, as the complex 
and sometimes long-range phenomena in natural language provides a good testbed.
%Additionally, language modeling captures import scientific and linguistic questions.
Language model benchmarks are currently dominated by autoregressive neural models,
which makes it difficult to analyze what properties of language the models are
actually capturing.
%For example, it is difficult to analyze the inner representations used
%in syntax-inspired neural language models \citep{du2020exploiting}
%(this is my own take on that paper,
%need to find a better citation). 
This is an issue if the goal is to learn from models in order to answer
scientific questions.

In this work, we investigate scaling probabilistic graphical models
which explicitly reason about latent variables.
We focus on the one of the simplest latent variable models,
the hidden Markov model (HMM).
HMMs posit a simple generative process, that first generates a sequence of
latent states then the emissions.
Additionally, HMMs make very strong conditional independence assumptions.
Despite the simplicity of HMMs,
they are still computationally expensive to scale
due to the cost of inference.

In order to scale HMMs, we propose a series of choices in parameterization
that result in computational speedups for inference in HMMs:
sparse emission constraint, state dropout, and an efficient kernel-based softmax.
Before diving into these techniques, we first review HMMs.

\section{Hidden Markov Models for Language Modeling}
Hidden Markov models (HMMs) have a rich history in natural language processing.
They have been used for a series of tasks, including
speech recognition \citep{rabiner1990tut},
part of speech tagging \citep{merialdo1994tagging}, 
and word alignment in machine translation \citep{vogel1996hmm}.
They have also been used as components in neural models,
such as in attention \citep{shankar2018posterior}.
We focus on applying HMMs to the task of language modeling,
where the goal is to model the tokens in a sentence $x = (x_1,\ldots,x_T)$.

HMMs are one of the simplest latent variable models for language modeling
with a per-token latent variable.
The simplicity is a result of the very strong conditional independence assumption:
Every timestep has a single discrete state latent variable for representing context,
and HMMs emit each token given only the corresponding latent state.
Additionally, the next state is a function of only the previous state.
These independence assumptions result in a bottleneck where all information
must flow through the single state variable,
causing performance to be limited by the number of states
while also giving the model interpretability by limiting dependencies.

Formally, HMMs have the following generative process:
for each $t \in [T]$ timestep, first choose a latent state $z_t \in \mcZ$,
where $|\mcZ|$ is the number of latent states,
then choose a token to emit $x_t \in \mcX$.
This defines the joint distribution:
\begin{equation}
p(x,z) = \prod_t p(x_t\mid z_t) p(z_t \mid z_{t-1}),
\end{equation}
with transitions $p(z_t \mid z_{t-1})$, emissions $p(x_t \mid z_t)$,
and a distinguished start state $z_0 = S$.
%where we have the transition matrix $A_{z_{t-1}z_t} = p(z_t \mid z_{t-1})$
%and emission matrix $O_{z_tx_t} = p(x_t \mid z_t)$.

Training an HMM requires marginalizing over the unobserved
$z = (z_1,\ldots,z_T)$ in order to obtain the evidence $p(x) = \sum_z p(x,z)$,
accomplished via the forward algorithm.
The forward algorithm can be written as a sequence of matrix-vector multiplications:
Let the transition operators $\Lambda_t\in\R^{|\mcZ|\times|\mcZ|}$ for $t\in[2,\ldots,T]$, 
with entries $$[\Lambda_t]_{z_t,z_{t-1}} = p(x_t\mid z_t)p(z_t\mid z_{t-1}),$$
with the first operator given by
$$[\Lambda_1]_{z_1,z_0} = \begin{cases}
p(x_1 \mid z_1) p(z_1 \mid z_0=S) & z_0 = S\\
0 & \textrm{otherwise}.
\end{cases}
$$
The evidence is then given by
\begin{equation}
p(x) = \mathbf{1}^\top\Lambda_1\Lambda_2\cdots\Lambda_T\mathbf{1},
\end{equation}
where $\mathbf{1}\in\R^{|\mcZ|\times1}$ is the column vector of all ones.

On a serial machine, the cost of computing each matrix-vector product in
the above equation takes time $O(|\mcZ|^2)$, resulting in a total
running time of $O(T|\mcZ|^2)$ for the forward algorithm.
\todo{gradient computation requires $O(T|\mcZ|^2)$ space}
The quadratic dependence on the number of states precludes
scaling to extremely large state spaces.

In the following sections we will introduce techniques that
aim to reduce the dependence of the time and space complexity
of inference on the size of the state space.

\section{Conditional Expectations with Generalized Softmax}
Parameterization of the probability distributions in a model affects both
computational performance as well as generalization.
Recent work on efficient attention in neural models shows that careful choice of parameterization
of the softmax kernel, the main component of attention,
results in large computational gains at little cost in accuracy
\citep{choromanski2020performer,peng2021rfa}.
In this section we will cover generalized softmax with kernels and the
efficient computation of conditional expectations with generalized softmax.

Softmax is commonly used to parameterize conditional distributions in
neural nets, and has origins in smooth
approximations of argmax as well as the conditional max entropy problem \citep{}.
In a conditional distribution $p(x \mid z)$, the softmax parameterization is as follows: 
$$p(x \mid z) = \frac{\exp(\bu_z^\top \bv_x)}{\sum_{x'} \exp(\bu_z^\top \bv_{x'})},$$
with $\bu_z,\bv_x\in\R^d$ vector-space embeddings of values of $x,z$.\footnote{
Softmax may also have a temperature parameter that rescales 
the dot product, which we omit.
}
The numerator, $\exp(\bu_z^\top \bv_x)$, is the exponential kernel \citep{rawat2019sampledsoftmax}.
Although the parameterization of softmax relies on the exponential kernel,
we can generalize softmax to arbitrary nonnegative kernels $K: \R^d\times\R^d\to\R_+$.
Replacing the exponential kernel in softmax with the nonnegative kernel $K$
gives us generalized softmax,\footnote{
The naming of generalized softmax is inspired by generalized linear models.
}
$$p(x \mid z) = \frac{K(\bu_z, \bv_x)}{\sum_{x'} K(\bu_z, \bv_{x'})},$$
where we again have embeddings $\bu_z,\bv_x\in\R^d$.

Kernels have a long history in machine learning,
where they are used to extend linear classification models to nonlinear feature spaces,
such as in support vector machines or regression.
In those settings, kernels are intuitively used to measure the similarity between two data points,
relying on the existence of a feature mapping in an associated reproducing kernel Hilbert space 
for kernel functions with the finitely positive semi-definite property \citep{}.
In our generalized softmax setting, kernels instead parameterize conditional distributions by
measuring the propensity of one random variable taking a particular value given
the value of another random variable.
However, the connection to feature maps in reproducing kernel Hilbert spaces 
is still useful.
In particular, the existence of a feature map $\phi:\R^d\to\mathbb{F}$,
which maps from the input space $\R^d$ to the feature space $\mathbb{F}$,
allows us to write generalized softmax in matrix form:
$$p(x \mid z) \propto [\phi(U)\phi(V)^\top]_{zx},$$
where $\phi(U) = [\phi(\bu_1), \phi(\bu_2), \ldots,] \in\R^{|\mcZ|\times \dim(\mathbb{F})}$
and $\phi(V) = [\phi(\bv_1), \phi(\bv_2), \ldots]\in\R^{|\mcX|\times \dim(\mathbb{F})}$.
We will rely on the dimension of the feature space $\dim(\mathbb{F}) < \min(|\mcZ|,|\mcX|)$
in order to speed up and reduce the space complexity of computing conditional expectations.

Many probabilistic queries can be written as conditional expectations.
In particular, we can write marginalization in an HMM as a conditional expectation:
$$p(x) = \sum_z p(x,z) = \sum_z p(z)p(x \mid z) = \Es{z}{p(x \mid z)}.$$
Decomposing the expectation over time and examining a single timestep, we have
$$p(x_t \mid \bx_{<t}) = \sum_{z_t} p(x_t \mid z_t)p(z_t \mid x_{<t})
= \Es{z_t\mid x_{<t}}{p(x_t \mid z_t)}.$$
We can use the matrix form of generalized softmax to speed up this computation.
Let $[F]_{z_t} = p(x_t\mid z_t)$ in
$$\Es{z_t \mid x_{<t}}{p(x_t \mid z_t)} = D\phi(U)\phi(V)^\top F$$

Unfortunately, in the case of the exponential kernel, the feature space $\mathbb{F}_{\exp}$
has infinite dimension.
This can be seen from the Taylor expansion:
\begin{align*}
\exp(\bu^\top \bv)
&= \sum_{n=0}^\infty \frac{(\bu^\top \bv)^n}{n!}\\
&= \sum_{n=0}^\infty \frac{(\sum_{i=1}^du_iv_i)^n}{n!}\\
&= \sum_{n=0}^\infty \frac{\sum_{\mathbf{j} \in [d]^n}(\prod_{i=1}^n u_{j_i})(\prod_{i=1}^n v_{j_i})}{n!},
\end{align*}
where we have expanded the terms of the numerator by summing over all different orderings
$\mathbf{j}$ of subsets of size $n$.
This yields a feature map of 
$$[\phi(\bu)]_{n,\mathbf{j}} = \frac{\prod_{i=1}^n u_{j_i}}{\sqrt{n!}}$$
for all $n\in\N$.\footnote{
Note that while this is one possible feature map, there may exist
multiple valid feature maps.
}
We can therefore express the exponential kernel as an inner product of
infinite dimensional vectors \citep{cotter2011gausskernel},
$$\exp(\bu^\top\bv) 
= \sum_{n=0}^\infty\sum_{\mathbf{j}\in[d]^n}[\phi(\bu)]_{n,\mathbf{j}}[\phi(\bv)]_{n,\mathbf{j}}
= \langle \phi(\bu), \phi(\bv) \rangle,
$$
where the dimension of the feature space $\dim(\mathbb{F}_{\exp}) = \dim(\phi(\bu)) = \infty$.
Since the infinite dimensional feature space of the exponential kernel is clearly larger than
$\min(|\mcZ|,|\mcX|)$, recent work has instead turned to approximations of the exponential kernel
based on random Fourier features
\citep{rawat2019sampledsoftmax,choromanski2020performer,peng2021rfa}.
These approaches use a finite-dimensional unbiased estimator of the exponential kernel.

We propose to take a different strategy, where we directly learn a feature map
while disregarding the explicit functional form of the kernel.

\section{Kernel Approximations}
Move to appendix

Taylor approximation, generating functions

limitations?
\subsection{Bochner Theorem}
\subsection{Nystrom}

\section{Kernelized Inference}

\section{Generalization: Kernelized Belief Propagation}

\section{Spectral Methods?}

\bibliographystyle{plainnat}
\bibliography{bib}

\begin{appendix}
\begin{comment}
\section{Gradient Estimator Implementation}
In the log-semiring, addition is given by $\bigoplus = \LSE$ and multiplication
by $\bigotimes = +$.
Consider the linear chain CRF, $\bigotimes_t \psi(x_{t-1}, x_t)$,
with $\psi(x_{t-1}, t) = f(x_t) \oplus g(x_{t-1},x_t)$.
We would like to compute the gradient of the
log partition function, $A = \bigoplus_x \bigotimes_t \psi(x_{t-1}, x_t)$.
Recall the gradient identities 
\begin{equation}
\begin{aligned}
\nabla_a a \bigoplus b &= \frac{\exp(a)}{\exp(a \bigoplus b)}\\
\nabla_a a \bigotimes b &= 1.
\end{aligned}
\end{equation}

We then have 
\begin{equation}
\begin{aligned}
\nabla_{\psi(x_a,x_b)} &\bigoplus_t \psi(x_{t-1}, x_t)\\
&= \nabla_{\psi(x_a,x_b)} \bigoplus_t \psi(x_{t-1}, x_t)\\
\end{aligned}
\end{equation}
\end{comment}

\end{appendix}

\end{document}
