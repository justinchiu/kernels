\documentclass{article}

\usepackage[a4paper,margin=1in]{geometry}

\usepackage{mystyle}
\usepackage[sort,comma,numbers]{natbib}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{algorithm}

\usepackage{appendix}

\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{subcaption}

\title{Scaling Hidden Markov Language Models}

\begin{document}
\maketitle

\abstract{Asdf}

\section{Introduction}
Neural network-based generative models have led to progress in
difficult tasks such as language modeling and machine translation.
However, this progress comes at the cost of interpretability.
Neural networks are flexible function approximations,
but that flexibility results in opaque models that are difficult to analyze.
Rather than post-hoc analysis \citep{probing}, we instead propose to explore
the space of models that are defined with interpretability in mind.
In particular, we focus on probabilistic graphical models \citep{},
which allow for the execution of arbitrary probabilistic queries,
i.e. the calculation of (functions of) probability distributions
via operations such as conditioning and marginalization \citep{koller} (simplify).
We seek to answer the question of whether we can have models that are both
performant and interpretable.

We use language modeling as a benchmark task, as the complex 
and sometimes long-range phenomena in natural language provides a good testbed.
Additionally, language modeling captures import scientific and linguistic questions
(more on this later).
Language model benchmarks are currently dominated by autoregressive neural models,
which makes it difficult to analyze what properties of language the models are
actually capturing.
For example, it is difficult to analyze the inner representations used
in syntax-inspired neural language models \citep{du2020exploiting}
(this is my own take on that paper,
need to find a better citation). 
This is an issue if the goal is to learn from models in order to answer
scientific questions.

In this work, we investigate scaling probabilistic graphical models
which explicitly reason about latent variables.
We focus on the one of the simplest latent variable models,
the hidden Markov model (HMM).
HMMs posit a simple generative process, that first generates a sequence of
latent states then the emissions.
Additionally, HMMs make very strong conditional independence assumptions.
Despite the simplicity and constraints of HMMs,
they are still computationally expensive to scale
due to the cost of marginalization.

In order to scale HMMs, we propose a series of choices in parameterization
that result in computational speedups for inference in HMMs:
sparse emission constraint, state dropout, and softmax kernel feature map approximation.

Before diving into these contributions, we first review HMMs.

\section{Hidden Markov Models for Language Modeling}
Hidden Markov models (HMMs) have a rich history in natural language processing.
Speech recognition \citep{rabiner1990tut},
part of speech tagging \citep{merialdo1994tagging}, 
and word alignment in machine translation \citep{vogel1996hmm}.
Have seen some use in neural attention-based models (cite posterior attention).
We focus on applying HMMs to the task of language modeling,
where the goal is to model the tokens in a sentence $x = (x_1,\ldots,x_T)$.

HMMs are cool because ?.

Formally, HMMs have the following generative process:
for each $t \in [T]$ timestep, first choose a latent state $z_t \in \mcZ$,
where $|\mcZ|$ is the number of latent states,
then choose a token to emit $x_t \in \mcX$.
This defines the joint distribution:
\begin{equation}
p(x,z) = \prod_t p(x_t\mid z_t) p(z_t \mid z_{t-1}),
\end{equation}
%where we have the transition matrix $A_{z_{t-1}z_t} = p(z_t \mid z_{t-1})$
%and emission matrix $O_{z_tx_t} = p(x_t \mid z_t)$.
with transitions $p(z_t \mid z_{t-1})$, emissions $p(x_t \mid z_t)$,
and a distinguished start state $z_0 = S$.

Training an HMM requires marginalizing over the unobserved
$z = (z_1,\ldots,z_T)$ in order to obtain the evidence $p(x) = \sum_z p(x,z)$,
accomplished via the forward algorithm.
The forward algorithm can be written as a sequence of matrix-vector multiplications:
Let the transition operators $\Lambda_t\in\R^{|\mcZ|\times|\mcZ|}$ for $t\in[2,\ldots,T]$, 
with entries $$[\Lambda_t]_{z_t,z_{t-1}} = p(x_t\mid z_t)p(z_t\mid z_{t-1}),$$
with the first operator given by
$$[\Lambda_1]_{z_1,z_0} = \begin{cases}
p(x_1 \mid z_1) p(z_1 \mid z_0=S) & z_0 = S\\
0 & \textrm{otherwise}.
\end{cases}
$$
The evidence is given by
\begin{equation}
p(x) = \mathbf{1}^\top\Lambda_1\Lambda_2\cdots\Lambda_T\mathbf{1},
\end{equation}
where $\mathbf{1}\in\R^{|\mcZ|\times1}$ is the column vector of all ones.

\section{Parameterization: Low Rank Decompositions}
Linear

\section{Kernels}
Nonlinear
Application to features, not datapoints.

Hilbert space, RKHS
Feature mapping
Empirical Mapping?

\subsection{Softmax}
Why is softmax / exponential kernel special?
Maximum entropy distributions, exponential family.
\href{https://www.lix.polytechnique.fr/~nielsen/CIG-slides.pdf}{Slides on information projections}
Power series representation?

\section{Softmax Approximations}
Taylor approximation, generating functions

limitations?

\section{Kernelized Inference}

\section{Generalization: Kernelized Belief Propagation}

\section{Spectral Methods}

\bibliographystyle{plainnat}
\bibliography{bib}

\begin{appendix}
\section{Gradient Estimator Implementation}
In the log-semiring, addition is given by $\bigoplus = \LSE$ and multiplication
by $\bigotimes = +$.
Consider the linear chain CRF, $\bigotimes_t \psi(x_{t-1}, x_t)$,
with $\psi(x_{t-1}, t) = f(x_t) \oplus g(x_{t-1},x_t)$.
We would like to compute the gradient of the
log partition function, $A = \bigoplus_x \bigotimes_t \psi(x_{t-1}, x_t)$.
Recall the gradient identities 
\begin{equation}
\begin{aligned}
\nabla_a a \bigoplus b &= \frac{\exp(a)}{\exp(a \bigoplus b)}\\
\nabla_a a \bigotimes b &= 1.
\end{aligned}
\end{equation}

We then have 
\begin{equation}
\begin{aligned}
\nabla_{\psi(x_a,x_b)} &\bigoplus_t \psi(x_{t-1}, x_t)\\
&= \nabla_{\psi(x_a,x_b)} \bigoplus_t \psi(x_{t-1}, x_t)\\
\end{aligned}
\end{equation}


\end{appendix}

\end{document}
